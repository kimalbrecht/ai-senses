<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Artificial Senses</title>
  <meta name="description" content="">
  <meta name="author" content="Kim Albrecht">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
<meta name="viewport" content="initial-scale=1, maximum-scale=1, user-scalable=0"/>
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="mobile-web-app-capable" content="yes">

  <!-- Open Graph
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
<meta name="author" content="Kim Albrecht">
<meta name="copyright" content="kimalbrecht.com">
<meta name="description" content="Artificial Senses visualizes sensor data of the machines that surround us to develop an understanding how they experience the world.">
<meta name="robots" content="index"/>
<meta property="og:title" content="Artificial Senses by Kim Albrecht">
<meta property="og:type" content="website">
<meta property="og:description" content="Artificial Senses visualizes sensor data of the machines that surround us to develop an understanding how they experience the world.">
<meta property="og:image" content="https://kimalbrecht.github.io/ai-senses/images/ai-senses-sensor-visualization-kim-albrecht.jpg">
<meta property="og:url" content="https://kimalbrecht.github.io/ai-senses/">

  <!-- Redirect to kimalbrecht.com
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
<meta http-equiv="refresh" content="0; URL='https://artificial-senses.kimalbrecht.com/'" />

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Lato:100,300,300i" rel="stylesheet">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/favicon.png">

  <!-- Piwik -->
  <script type="text/javascript">
    var _paq = _paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u="//kimalbrecht.com/piwik/";
      _paq.push(['setTrackerUrl', u+'piwik.php']);
      _paq.push(['setSiteId', '1']);
      var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
      g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
    })();
  </script>
  <!-- End Piwik Code -->
  
</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="row" style="margin-top: 8%">
      <div class="ten columns offset-by-two">
        <h1 id="headline">Artificial Senses</h1>
      </div>
    </div>
    <div class="row">
      <div class="five columns offset-by-one">
        <p class="introDescr">Artificial Senses visualizes sensor data of the machines that surround us to develop an understanding how they experience the world.</p>
      </div>
      <div class="six columns">
        <p>In current times, machine learning and artificial intelligence are buzzwords. But they are more than that—they influence our behavior as well as our conception  of the technologies themselves and the world they represent. A lack of understanding of how these systems operate on their own terms is dangerous. How can we live with, trust, and interact with this alien species, which we set forth into the world, if we know it only through interfaces designed to make the machine unnaturally akin to the world we already know? This project visualizes raw sensor data that our phones and computers collect and process, to help us understand how these machines experience the world.</p>
      </div>
    </div>

    <div class="grid"> <!-- Sensor Grid -->
      <div class="grid-item" id="locating">
        <video class="videoInsert" id="locatingVideo" poster="images/intro/cover/ai-senses-kim-albrecht-locating.jpg" loop webkit-playsinline>
          <source src="images/intro/ai-senses-kim-albrecht-locating.mp4" type="video/mp4">
        </video>
        <div class="sensorHeadline"><h6>Locating <span id="liveLocating" class="liveTag">live</span></h6></div>
        <div class="description" id="locatingDescr">
          <p class="sensorDescr">
            Each vertical line represents one request of the latitudinal and longitudinal geolocation of the device. Rather than displaying the two numbers that are returned the visualization displays every digit of those numbers individually. What becomes visible is the accuracy on which the device locates itself. It is not uncommon that devices return their location with an accuracy to the sixth or seventh decimal place. This means the device can know its position to an accuracy of a few centimeters/inches. The interesting part is that on every request, the location of the instrument, even when it is not in motion, is constantly changing. The visualization makes visible this constant guessing of the machine. Each digit of the latitude and longitude values are represented by saturation of blue and magenta. Both created patterns are overlaid: the difference between the bottom layer and the top layer is calculated is represented as a positive value.
          </p>
          <p class="sensorDescr">
            While the location sensor works on many devices and browsers, it often behaves very differently. Try different machines and web browsers.
          </p>
          <p class="goLive" id="locatingLiveButton">Open Live Visualization</p>
        </div>
      </div>

      <div class="grid-item" id="moving">
        <video class="videoInsert" id="movingVideo" poster="images/intro/cover/ai-senses-kim-albrecht-moving.jpg" loop webkit-playsinline>
          <source src="images/intro/ai-senses-kim-albrecht-moving.mp4" type="video/mp4">
        </video>
        <div class="sensorHeadline"><h6>Moving <span id="liveMoving" class="liveTag">live</span></h6></div>
        <div class="description" id="movingDescr">
          <p class="sensorDescr">
            This visualization represents the device motion sensor. Each dot stands for one digit returned by the apparatus. The longer the returned numbers, the more points from top to bottom of each row. Darker lines symbolize negative numbers, brighter lines symbolize positive numbers. Time is drawn from left to right for each row.
          </p>
          <p class="sensorDescr">
            Motion sensors are most common in smartphones and tablets. Only very few personal computers have them. If you are using a phone or a tablet and you can not see the live visualization try switching browsers.
          </p>
          <p class="goLive" id="movingLiveButton">Open Live Visualization</p>
        </div>
      </div>
      
      <div class="grid-item" id="seeing">
        <video class="videoInsert" id="seeingVideo" poster="images/intro/cover/ai-senses-kim-albrecht-seeing.jpg" loop webkit-playsinline>
          <source src="images/intro/ai-senses-kim-albrecht-seeing.mp4" type="video/mp4">
        </video>
        <div class="sensorHeadline"><h6>Seeing <span id="liveSeeing" class="liveTag">live</span></h6></div>
        <div class="description" id="seeingDescr">
          <p class="sensorDescr">
            The code behind the visualization captures an image from the camera of the device. This picture gets stored in the JPEG format encoded in base64. Base64 is a binary-to-text encoding scheme: A = 000000, B = 000001, C = 000010. The total encoding holds 64 characters with a one-to-one mapping into binary code. The visualization maps these 64 characters to 64 grayscale values between black and white, and displays each value on the screen from left to right and top to bottom. This process repeats every second.
          </p>
          <p class="sensorDescr">
            While many phones and tablets have a camera their access over the web today often only works on personal computers. If you are on a personal computer and the sensor is not live, try switching browsers.
          </p>
          <p class="goLive" id="seeingLiveButton">Open Live Visualization</p>
        </div>
      </div>
      
      <div class="grid-item" id="hearing">
        <video class="videoInsert" id="hearingVideo" poster="images/intro/cover/ai-senses-kim-albrecht-hearing.jpg" loop webkit-playsinline>
          <source src="images/intro/ai-senses-kim-albrecht-hearing.mp4" type="video/mp4">
        </video>
        <div class="sensorHeadline"><h6>Hearing <span id="liveHearing" class="liveTag">live</span></h6></div>
        <div class="description" id="hearingDescr">
          <p class="sensorDescr">
            The code captures the frequency data of the microphone of the device. Each frequency value ranges between 0 and 255. The number of frequencies collected can range between 32 and 32768. Frequencies are drawn from left (low frequency), to the right (high-frequency). The range of each frequency is illustrated by each point from 0 (black) to 255 (white). This data is requested 20 times per second and time moves from top to bottom.
          </p>
          <p class="sensorDescr">
            Microphone web access is restricted on most phones and tablets. Most probably this sensor will work on your laptop or pc. 
          </p>
          <p class="goLive" id="hearingLiveButton">Open Live Visualization</p>
        </div>
      </div>

      <div class="grid-item" id="touching">
        <video class="videoInsert" id="touchingVideo" poster="images/intro/cover/ai-senses-kim-albrecht-touching.jpg" loop webkit-playsinline>
          <source src="images/intro/ai-senses-kim-albrecht-touching.mp4" type="video/mp4">
        </video>
        <div class="sensorHeadline"><h6>Touching <span id="liveTouching" class="liveTag">live</span></h6></div>
        <div class="description" id="touchingDescr">
          <p class="sensorDescr">
            Each vertical row represents the vertical and horizontal position of the finger on the screen of the device compared against the total height and width of the screen. Rather than displaying the two numbers representative of the location of the user’s finger, the visualization uses every digit of those often long decimal places individually. The width of each drawn line depends on the time between two incoming signals. Both created patterns are overlaid onto each other. The difference between the bottom and top layer is calculated to always return a positive value.
          </p>
          <p class="sensorDescr">
            As ‘touch’ in the form of mouse gestures or screen contact is such a common gesture in graphical user interfaces, this visualization will work on nearly all devices. 
          </p>
          <p class="goLive" id="touchingLiveButton">Open Live Visualization</p>
        </div>
      </div>

      <div class="grid-item" id="orienting">
        <video class="videoInsert" id="orientingVideo" poster="images/intro/cover/ai-senses-kim-albrecht-orienting.jpg" loop webkit-playsinline>
          <source src="images/intro/ai-senses-kim-albrecht-orienting.mp4" type="video/mp4">
        </video>
        <div class="sensorHeadline"><h6>Orienting <span id="liveOrienting" class="liveTag">live</span></h6></div>
        <div class="description" id="orientingDescr">
          <p class="sensorDescr">
            This graphic visually recodes the gyro sensor that is built into most contemporary smartphones. Similar to the LOCATING visualization, each individual digit is represented – rather than the entire number – as there is very high accuracy in the sensory output. The x-axis of the gyroscope returns the direction the phone is pointing toward as a number between 0 and 360. But rather than returning an integer, depending on the device, the number can have more than 20 decimal places. The machine returns the direction of the phone on a level of air vibrations.
          </p>
          <p class="sensorDescr">
            Most phones and tablets nowadays have a gyro sensor. It will be harder to find in personal computers.
          </p>
          <p class="goLive" id="orientingLiveButton">Open Live Visualization</p>
        </div>
      </div>
      
    </div> <!-- Sensor Grid -->
    
    <div id="row" style="margin-top: 10%;"> <!-- Descr. Long -->
      <div class="twelve columns offset-by-two">
        <p class="six columns">
          Contemporary culture is unimaginable without the machines that surround us every day. Our knowledge is influenced by Google search results, our music taste by the mixes Spotify creates for us, and our shopping choices by Amazon recommendations. This strange new world became part of our reality in a very short time. Human-facing interface design makes these systems feel natural, as if they are really of our world. But if we want to live with these devices and understand them, we should not soley rely on the machines becoming something easily understandable to us. <b><i>We need to develop an understanding of how these devices experience our world.</i></b>
        </p>
      </div>
      <div class="twelve columns offset-by-five">
        <p class="six columns">
          The visualizations here explore a number of sensory domains: seeing, locating, orienting, hearing, moving, and touching. Rather than yielding machine’s sensory data in ways that we intuitively grasp, however, these visualization try to get closer to the machine’s experience. They show us a number of ways in which the machine’s reality departs from our own. With many of its sensors, for example, the machine is operating in a timescale that is too fast to understand; the orientation sensor returns data up to 300 times per second. This is too quick  to draw each of these values on the screen, and also too quick for us to comprehend. In most cases, to make these visualizations, the machine had to be tamed and slowed for us to perceive its “experience.”
        </p>
      </div>
    </div>
    <div id="row">
      <div class="twelve columns offset-by-two">
        <p class="six columns">
          A second and more worrying finding is the similarity among many of the images. Seeing, hearing, and touching, for humans, are qualitatively different experiences of the world; they lead to a wide variety of understandings, emotions, and beliefs. For the machine, these senses are very much the same, reducible to strings of numbers with a limited range of actual possibilities. While some of these sensory experiences—notably temperature—have long been given numerical value, their effects on us remain ineffable. Nowadays, however, it is not only temperature that can be reduced to a discrete number, but seemingly anything. But is this really true? Isn’t there something that our current measures of temperature does not reveal about the entire spectrum, from crisp cold to feverishly hot? And is this a question of more data points, or is there a deeper disconnect, reflective of a difference in kind, in these translations?
        </p>
      </div>
      <div class="twelve columns offset-by-five">
        <p class="six columns">
          The entire orientation of a machine towards the world is mediated by numbers. For the machine, reality is binary—a torrent of on and off. Any knowledge about the world that we learn from the machine goes through this process of abstraction. As we become more dependent on our machines, we need to understand the underlying limits and boundaries of this abstraction.
        </p>
      </div>
    </div> 

    <div id="row">
      <div class="eight columns offset-by-three" style="margin-top: 10%; margin-bottom: 10%;">
        <h5>Further Explorations</h5>
        <div class="eight columns offset-by-one" >
          <a href="harvard_art_museums_exhibit.html"><p style="font-size: 120%; margin-bottom: 10px;">Harvard Art Museums exhibition</p></a>
          <p>Images form the Machine Experience exhibition at the Harvard Art Museums.</p>
        </div>
        <div class="eight columns offset-by-one" >
          <a href="sensor_mappings.html"><p style="font-size: 120%; margin-bottom: 10px;">Sensor Mappings</p></a>
          <p>Visual explainations of the mappings from sensor data to visual output.</p>
        </div>
      </div>
    </div>

    <div id="row">
      <div class="eight columns offset-by-three" style="margin-bottom: 10%;">
        <h5>Selected Press Coverage</h5>
        <div class="eight columns offset-by-one" >
          <a href="https://www.wired.com/story/see-the-world-through-the-eyes-of-your-phone/"><p style="font-size: 120%; margin-bottom: 10px;">WIRED</p></a>
          <p>'See the World through the Eyes of your Phone'</p>
        </div>
        <div class="eight columns offset-by-one" >
          <a href="https://www.fastcodesign.com/90144612/these-eerie-gifs-show-how-your-phone-feels-hears-and-sees-you"><p style="font-size: 120%; margin-bottom: 10px;">Fast Company</p></a>
          <p>'These Eerie GIFs Show How Your Phone Feels, Hears, And Sees You'</p>
        </div>
      </div>
    </div> 

    <!-- Descr. Long -->

    <div id="row" style="margin-top: 10%;"> <!-- Footer -->
        <div class="eight columns offset-by-two" style="border-top: 1px solid #eee;">
            <div id="row" style="margin: 5%;">
              <div class="nine columns">
                <p>
                  Artificial Senses is a project by <a href="http://kimalbrecht.com/">Kim Albrecht</a> in collaboration with <a href="http://metalab.harvard.edu/">metaLAB (at) Harvard</a>, and supported by the <a href="https://cyber.harvard.edu/research/ai">Berkman Klein Center for Internet &amp; Society</a>. The project is part of a larger initiative researching the boundaries between artificial intelligence and society.
                </p>
                <p>
                  Contact via <a href="http://kimalbrecht.com/">website</a> or <a href="https://twitter.com/kimay">twitter</a>
                </p>
                <p>
                  <small>Copyright &copy; 2017 Kim Albrecht all rights reserved</small>
                </p>
                
              </div>
              <div class="three columns">
                  <img src="images/ml_logo.gif" style="width:100%; max-width:100px;">
              </div>
            </div>
        </div>
    </div> <!-- Footer -->

  </div>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->

<!-- JS
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src="libs/isotope.pkgd.min.js"></script>

  <script src="js/open-vis.js"></script>
  <script src="js/layout-mode.js"></script>
  <script src="js/sensor-check.js"></script>
  
</body>
</html>
