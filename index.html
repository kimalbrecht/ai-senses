<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>AI Senses</title>
  <meta name="description" content="">
  <meta name="author" content="Kim Albrecht">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
<meta name="viewport" content="initial-scale=1, maximum-scale=1, user-scalable=0"/>
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="mobile-web-app-capable" content="yes">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Lato:100,300,300i" rel="stylesheet">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/favicon.png">

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="row" style="margin-top: 8%">
      <div class="ten columns offset-by-two">
        <h1 id="headline">AI Senses</h1>
      </div>
    </div>
    <div class="row">
      <div class="five columns offset-by-one">
        <p class="introDescr">Ai Senses visualizes sensor data of machines that surround us on a daily basis to develop an understanding how they experience the world.</p>
      </div>
      <div class="six columns">
        <p class="sensorDescr">In current times, “machine learning” and “artificial intelligence” are buzzwords. But they are more than that—they influence our behaviors and understandings of the technologies they describe and the world they make. A lack of understanding of how these systems operate on their own terms is dangerous. How can we live and interact with this alien species, which we set forth into the world, if we know it through interfaces constructed to make the machine feel closer to the world we already know? This project visualizes sensor data that our cell phones and personal computers collect and digest on our behalf, to help us understand how these machines experience the world.</p>
      </div>
    </div>

    <div class="grid"> <!-- Sensor Grid -->
      <div class="grid-item" id="locating">
        <video class="videoInsert" id="locatingVideo" poster="images/intro/cover/ai-senses-kim-albrecht-locating.jpg" loop webkit-playsinline>
          <source src="images/intro/ai-senses-kim-albrecht-locating.mp4" type="video/mp4">
        </video>
        <div class="sensorHeadline"><h6>Locating <span id="liveLocating" class="liveTag">live</span></h6></div>
        <div class="description" id="locatingDescr">
          <p class="sensorDescr">
            Each vertical line represents one request of the latitude and longitude geolocation of the device. Rather than displaying the two numbers that are returned the visualization displays every digit of those numbers individually. What becomes visible is the accuracy on which the device locates itself. It is not uncommon that devices return their location with an accuracy to the sixth or seventh decimal place. This means the device returns its position to an accuracy of a few centimeters. The interesting part is that on every request the location of the device, even when it is not in motion, is constantly changing. The visualization makes these constant guessing of the machine visible. Each digit of the latitude and longitude values are represented by saturations of blue and magenta. Both created patterns are overlaid onto each other. Either the bottom layer is subtracted from the top layer or the other way around to always return a positive value.
          </p>
          <p class="goLive" id="locatingLiveButton">Open Live Visualization</p>
        </div>
      </div>

      <div class="grid-item" id="moving">
        <video class="videoInsert" id="movingVideo" poster="images/intro/cover/ai-senses-kim-albrecht-moving.jpg" loop webkit-playsinline>
          <source src="images/intro/ai-senses-kim-albrecht-moving.mp4" type="video/mp4">
        </video>
        <div class="sensorHeadline"><h6>Moving <span id="liveMoving" class="liveTag">live</span></h6></div>
        <div class="description" id="movingDescr">
          <p class="sensorDescr">
            The code captures the frequency data of the microphone of the device. Each frequency value ranges between 0 and 255. The number of frequencies collected can range between 32 and 32768. This data is requested 20 times per second and drawn on the screen as values between black and white.
          </p>
          <p class="goLive" id="movingLiveButton">Open Live Visualization</p>
        </div>
      </div>
      
      <div class="grid-item" id="seeing">
        <video class="videoInsert" id="seeingVideo" poster="images/intro/cover/ai-senses-kim-albrecht-seeing.jpg" loop webkit-playsinline>
          <source src="images/intro/ai-senses-kim-albrecht-seeing.mp4" type="video/mp4">
        </video>
        <div class="sensorHeadline"><h6>Seeing <span id="liveSeeing" class="liveTag">live</span></h6></div>
        <div class="description" id="seeingDescr">
          <p class="sensorDescr">
            The code behind the visualization captures an image from the camera of the device. This picture gets stored in the JPEG format encoded in base64. Base64 is a binary-to-text encoding scheme: A = 0000, B = 000001, C = 000010. The total encoding holds 64 characters with a one to one mapping into binary code. The visualization maps this 64 characters to 64 values between black and white and draws each character on the screen from left to right and top to bottom. This process repeats every second.
          </p>
          <p class="goLive" id="seeingLiveButton">Open Live Visualization</p>
        </div>
      </div>
      
      <div class="grid-item" id="hearing">
        <video class="videoInsert" id="hearingVideo" poster="images/intro/cover/ai-senses-kim-albrecht-hearing.jpg" loop webkit-playsinline>
          <source src="images/intro/ai-senses-kim-albrecht-hearing.mp4" type="video/mp4">
        </video>
        <div class="sensorHeadline"><h6>Hearing <span id="liveHearing" class="liveTag">live</span></h6></div>
        <div class="description" id="hearingDescr">
          <p class="sensorDescr">
            The code captures the frequency data of the microphone of the device. Each frequency value ranges between 0 and 255. The number of frequencies collected can range between 32 and 32768. This data is requested 20 times per second and drawn on the screen as values between black and white.
          </p>
          <p class="goLive" id="hearingLiveButton">Open Live Visualization</p>
        </div>
      </div>

      <div class="grid-item" id="touching">
        <video class="videoInsert" id="touchingVideo" poster="images/intro/cover/ai-senses-kim-albrecht-touching.jpg" loop webkit-playsinline>
          <source src="images/intro/ai-senses-kim-albrecht-touching.mp4" type="video/mp4">
        </video>
        <div class="sensorHeadline"><h6>Touching <span id="liveTouching" class="liveTag">live</span></h6></div>
        <div class="description" id="touchingDescr">
          <p class="sensorDescr">
            The code captures the frequency data of the microphone of the device. Each frequency value ranges between 0 and 255. The number of frequencies collected can range between 32 and 32768. This data is requested 20 times per second and drawn on the screen as values between black and white.
          </p>
          <p class="goLive" id="touchingLiveButton">Open Live Visualization</p>
        </div>
      </div>

      <div class="grid-item" id="orienting">
        <video class="videoInsert" id="orientingVideo" poster="images/intro/cover/ai-senses-kim-albrecht-orienting.jpg" loop webkit-playsinline>
          <source src="images/intro/ai-senses-kim-albrecht-orienting.mp4" type="video/mp4">
        </video>
        <div class="sensorHeadline"><h6>Orienting <span id="liveOrienting" class="liveTag">live</span></h6></div>
        <div class="description" id="orientingDescr">
          <p class="sensorDescr">
            This graphic visually recodes the gyro sensor that is build into most contemporary smartphones. Similar to the locating visualization each digit is represented rather than the entire number as there is a very high accuracy in the sensory output. The x axis of the gyroscope returns the direction the phone is pointing towards as a number between 0 and 360. But rather than returning a integer, depending of the device, the number can have more than 20 decimal places. The machine returns the direction of the phone on a level of air vibrations
          </p>
          <p class="goLive" id="orientingLiveButton">Open Live Visualization</p>
        </div>
      </div>
      
    </div> <!-- Sensor Grid -->
    
    <div id="row" style="margin-top: 5%;"> <!-- Descr. Long -->
      <div class="four columns offset-by-two">
        <p style="text-align: end;">
          Contemporary culture is unimaginable without the machines that surround us every day. Our knowledge depends on Google search results, our music taste on the mixes Spotify creates for us and our consumption on Amazon recommendations. This strange new world became part of our reality in a very short timeframe. Companies that are part of this economy developed strategies that are turning these new creatures into something that feels natural to us, ’It just works’ as Steve Jobs liked to say. Interface design is the discipline that turns the machine into something that creates this natural feeling. But if we want to live with these devices and understand them we can not just rely on the machines becoming something easily understandable to us. We need to develop an understanding of how these devices experience our world. With machine learning and artificial intelligence being such buzzwords nowadays not having an understanding of how these machines experience the world is dangerous. How can we live and interact with this for us so alien species, that we set into the world if the only thing we know about it are these constructed interfaces which make the machine feel so close to the world we already know?
        </p>
      </div>
      <div class="four columns">
        <p>
          Computer Human Interfaces usually try to be as human as possible. In this series of works, the interfaces seek to be close to the machine view as possible. Creating images that are strange and difficult to interpret for the human viewer but entirely natural for the machine.
          <br>
          This project looks at sensors that are part of our cell phones and personal computers to develop an understanding how the machines that surround us on a daily basis experience the world. Usually, we do have contact with this sensors in a manner that suits our needs. We experience the gyro sensors to play video games, the camera for a Skype call, the location sensor for our map services. But these applications show us useful abstractions and not what the sensors detect. The collection of here presented visualizations is an aestheticization of the rawest form of data our internet browsers receive. These graphics are not meant to have a utilitarian usage. They are introduced into the world to give us a glimpse into the world of our machines.
        </p>
      </div>
    </div> <!-- Descr. Long -->

    <div id="row" style="margin-top: 10%;"> <!-- Footer -->
        <div class="eight columns offset-by-two" style="border-top: 1px solid #eee;">
            <div id="row" style="margin: 5%;">
              <div class="nine columns">
                <p>
                  AI Senses is a project by by <a href="http://kimalbrecht.com/">Kim Albrecht</a> in collaboration with the <a href="http://metalab.harvard.edu/">metaLAB (at) Harvard</a> and supported by the <a href="">Berkman Klein Center for Internet &amp; Society</a>. The project is part of a larger initiative researching the boundaries between artificial intelligence and society.
                    <br><br>
                    <small>Copyright &copy; 2017 Kim Albrecht all rights reserved</small>
                </p>
              </div>
              <div class="two columns offset-by-one">
                  <img src="images/ml_logo.gif" style="max-width:100%;">
              </div>
            </div>
        </div>
        
    </div> <!-- Footer -->

  </div>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->

<!-- JS
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src="libs/isotope.pkgd.min.js"></script>

  <script src="js/open-vis.js"></script>
  <script src="js/layout-mode.js"></script>
  <script src="js/sensor-check.js"></script>
  
</body>
</html>
